{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b0a541",
   "metadata": {},
   "source": [
    "# **Recursive Feature Elimination with Random Forest Selecting features for model performance**\n",
    "\n",
    " ## Table of contents\n",
    ">1. [Dataset](#Dataset)\n",
    ">2. [Random Forest Classifier](#Random_Forest_Classifier)\n",
    ">3. [Recursive feature elemination - Random forest](#Recursive_feature_elemination_Random_forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14398892",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a15061f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66432440",
   "metadata": {},
   "source": [
    "## Dataset <a name=\"Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f33a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/dataset/NoQC/X_train.csv').to_numpy()\n",
    "y_train = pd.read_csv('../data/dataset/NoQC/y_train.csv').to_numpy()\n",
    "X_test = pd.read_csv('../data/dataset/NoQC/X_test.csv').to_numpy()\n",
    "y_test = pd.read_csv('../data/dataset/NoQC/y_test.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "344fac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X_train =  (111, 605) y_train =  (111, 1)\n",
      "Testing data: X_test =  (28, 605) y_test =  (28, 1)\n"
     ]
    }
   ],
   "source": [
    "#check dataset\n",
    "print(\"Training data: X_train = \",X_train.shape, \"y_train = \", y_train.shape)\n",
    "print(\"Testing data: X_test = \",X_test.shape, \"y_test = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02d95c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c6860",
   "metadata": {},
   "source": [
    "## Random Forest Classifier <a name=\"Random_Forest_Classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e185c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    "    # Apply scale datasets\n",
    "    # create model scale ==> standard scaler\n",
    "    scaler = StandardScaler()   \n",
    "    # fit dataset to model scale\n",
    "    X_train_scl = scaler.fit_transform(X_train)\n",
    "    X_test_scl = scaler.fit_transform(X_test)\n",
    "    \n",
    "    # convert matrix 2d -> 1d (by flatten method)    \n",
    "    y_train = y_train.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "    \n",
    "    # Random forest classifier model\n",
    "    rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "    rf.fit(X_train_scl, y_train)\n",
    "        \n",
    "    # check classes\n",
    "    print(\"classes model: \")\n",
    "    print(rf.classes_)\n",
    "    \n",
    "    # Predict testing dataset\n",
    "    y_pred = rf.predict(X_test)\n",
    "    # Predict probability (1, 2) \n",
    "    y_score = rf.predict_proba(X_test)\n",
    "    print(y_score)\n",
    "    \n",
    "    # ROC curve: receiver operating characteristic curve\n",
    "    # AUC: Area Under the ROC Curve \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score[:,1], pos_label=2)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "#     tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "#     confusion = (tn, fp, fn, tp)\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    return rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad45f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes model: \n",
      "[1 2]\n",
      "[[0.59 0.41]\n",
      " [0.54 0.46]\n",
      " [0.62 0.38]\n",
      " [0.48 0.52]\n",
      " [0.59 0.41]\n",
      " [0.62 0.38]\n",
      " [0.57 0.43]\n",
      " [0.65 0.35]\n",
      " [0.53 0.47]\n",
      " [0.56 0.44]\n",
      " [0.53 0.47]\n",
      " [0.61 0.39]\n",
      " [0.44 0.56]\n",
      " [0.51 0.49]\n",
      " [0.7  0.3 ]\n",
      " [0.5  0.5 ]\n",
      " [0.55 0.45]\n",
      " [0.49 0.51]\n",
      " [0.56 0.44]\n",
      " [0.42 0.58]\n",
      " [0.69 0.31]\n",
      " [0.66 0.34]\n",
      " [0.6  0.4 ]\n",
      " [0.51 0.49]\n",
      " [0.64 0.36]\n",
      " [0.45 0.55]\n",
      " [0.53 0.47]\n",
      " [0.62 0.38]]\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check efficiency random forest classifier for all dataset (train, test)\n",
    "rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion = random_forest_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bd94f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score = 0.38802083333333337\n"
     ]
    }
   ],
   "source": [
    "print(\"auc_score = \"+ str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba6c9e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0),\n",
       " (1, 8.47975553857907e-05),\n",
       " (2, 0.0),\n",
       " (3, 0.0002404158544509422),\n",
       " (4, 0.0),\n",
       " (5, 0.00037525354969574165),\n",
       " (6, 0.0),\n",
       " (7, 0.004526001999516245),\n",
       " (8, 0.001857925037690587),\n",
       " (9, 0.0003629978684550598),\n",
       " (10, 0.004399380603239685),\n",
       " (11, 0.00204334628154042),\n",
       " (12, 0.0038685054970028247),\n",
       " (13, 0.001143099004319998),\n",
       " (14, 0.0019266384822382418),\n",
       " (15, 0.00889783928726194),\n",
       " (16, 0.0026729969656460882),\n",
       " (17, 0.004505636562028702),\n",
       " (18, 0.0),\n",
       " (19, 0.002060010009764487),\n",
       " (20, 0.0),\n",
       " (21, 0.0),\n",
       " (22, 0.0),\n",
       " (23, 0.004581500797855939),\n",
       " (24, 0.0068979842755490395),\n",
       " (25, 0.0),\n",
       " (26, 0.00449119878325455),\n",
       " (27, 0.00029365079365079364),\n",
       " (28, 0.0),\n",
       " (29, 0.0),\n",
       " (30, 0.009011324992868105),\n",
       " (31, 0.004146511897527134),\n",
       " (32, 0.0),\n",
       " (33, 0.001109496564669569),\n",
       " (34, 0.00789860705029066),\n",
       " (35, 0.0010263924256141389),\n",
       " (36, 0.0),\n",
       " (37, 0.0),\n",
       " (38, 0.006391928747776974),\n",
       " (39, 0.0),\n",
       " (40, 0.0),\n",
       " (41, 0.011665518870973637),\n",
       " (42, 0.0004221033868092693),\n",
       " (43, 0.0001989247311827958),\n",
       " (44, 0.00017583124754423404),\n",
       " (45, 0.0),\n",
       " (46, 0.005700543569232025),\n",
       " (47, 0.0),\n",
       " (48, 0.0),\n",
       " (49, 0.0),\n",
       " (50, 0.0),\n",
       " (51, 0.0),\n",
       " (52, 0.0),\n",
       " (53, 0.005605431644658461),\n",
       " (54, 0.007870034377194194),\n",
       " (55, 0.00029861286254728904),\n",
       " (56, 0.00040284733966805075),\n",
       " (57, 0.0),\n",
       " (58, 0.0),\n",
       " (59, 0.0),\n",
       " (60, 0.0),\n",
       " (61, 0.0),\n",
       " (62, 0.006247625553345409),\n",
       " (63, 0.0),\n",
       " (64, 0.0011732724996365886),\n",
       " (65, 0.00027281746031746206),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (68, 0.0007826455132993188),\n",
       " (69, 0.0003207249802994489),\n",
       " (70, 0.0002688143665723838),\n",
       " (71, 0.0),\n",
       " (72, 0.00442702059880124),\n",
       " (73, 0.0),\n",
       " (74, 0.0),\n",
       " (75, 0.0),\n",
       " (76, 0.0),\n",
       " (77, 0.0013780996089357645),\n",
       " (78, 0.0033411648462032787),\n",
       " (79, 0.0),\n",
       " (80, 0.0),\n",
       " (81, 0.001473773036904195),\n",
       " (82, 0.000736197580677173),\n",
       " (83, 0.00018743667679837903),\n",
       " (84, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (87, 0.0),\n",
       " (88, 0.0),\n",
       " (89, 0.0006680913624190935),\n",
       " (90, 0.0006509289897114438),\n",
       " (91, 3.032786885245903e-05),\n",
       " (92, 0.0),\n",
       " (93, 0.0),\n",
       " (94, 0.0),\n",
       " (95, 0.00023223363419441873),\n",
       " (96, 0.0004718282368249835),\n",
       " (97, 0.00024378030782205355),\n",
       " (98, 0.0),\n",
       " (99, 0.0),\n",
       " (100, 0.0),\n",
       " (101, 0.0),\n",
       " (102, 0.0003295147020556849),\n",
       " (103, 0.0032457538000745583),\n",
       " (104, 0.0),\n",
       " (105, 0.0),\n",
       " (106, 0.0007705388973771326),\n",
       " (107, 0.0),\n",
       " (108, 0.000279206510190283),\n",
       " (109, 0.012225196054643263),\n",
       " (110, 0.01563127582115065),\n",
       " (111, 0.0),\n",
       " (112, 0.002845025589048266),\n",
       " (113, 0.0),\n",
       " (114, 0.014698056176738996),\n",
       " (115, 0.009763416806037931),\n",
       " (116, 0.005903016603124396),\n",
       " (117, 0.00024109599326400708),\n",
       " (118, 0.005753054650508807),\n",
       " (119, 0.0),\n",
       " (120, 0.000214175456650373),\n",
       " (121, 0.0),\n",
       " (122, 0.006003320754618446),\n",
       " (123, 0.0),\n",
       " (124, 0.005296323944201637),\n",
       " (125, 0.005065462672593194),\n",
       " (126, 0.0),\n",
       " (127, 0.0),\n",
       " (128, 0.0),\n",
       " (129, 0.0067780267968286),\n",
       " (130, 0.0),\n",
       " (131, 0.00030228758169934645),\n",
       " (132, 0.017624535977725196),\n",
       " (133, 0.0),\n",
       " (134, 0.0005784387265878148),\n",
       " (135, 0.0008005833089400542),\n",
       " (136, 0.0011475646780163917),\n",
       " (137, 0.0),\n",
       " (138, 0.0),\n",
       " (139, 0.0),\n",
       " (140, 0.0),\n",
       " (141, 0.0004690468429875218),\n",
       " (142, 0.002721877775872547),\n",
       " (143, 0.0006384692077613314),\n",
       " (144, 0.0),\n",
       " (145, 0.0),\n",
       " (146, 0.0),\n",
       " (147, 0.0004802450984673645),\n",
       " (148, 0.0),\n",
       " (149, 0.011177275014258336),\n",
       " (150, 0.0),\n",
       " (151, 0.007917767880397894),\n",
       " (152, 0.002550275912422965),\n",
       " (153, 0.012255683808618911),\n",
       " (154, 0.005304996729736281),\n",
       " (155, 0.007135762015509411),\n",
       " (156, 0.0005201459243424113),\n",
       " (157, 0.008044715496184891),\n",
       " (158, 0.008351604565017592),\n",
       " (159, 0.0),\n",
       " (160, 0.0004464555052790345),\n",
       " (161, 0.005432979833776292),\n",
       " (162, 0.00023499624815579738),\n",
       " (163, 0.0),\n",
       " (164, 0.0003276269185360095),\n",
       " (165, 0.0),\n",
       " (166, 0.0002729508196721312),\n",
       " (167, 0.0),\n",
       " (168, 0.0),\n",
       " (169, 0.0),\n",
       " (170, 0.0),\n",
       " (171, 0.0),\n",
       " (172, 0.006085179024998635),\n",
       " (173, 0.0005428786023192623),\n",
       " (174, 0.0),\n",
       " (175, 0.0),\n",
       " (176, 0.006814339385125424),\n",
       " (177, 0.0),\n",
       " (178, 0.0),\n",
       " (179, 0.0),\n",
       " (180, 0.0),\n",
       " (181, 0.0),\n",
       " (182, 0.0002587964039370688),\n",
       " (183, 0.0),\n",
       " (184, 0.00012059973924380707),\n",
       " (185, 0.0003249837345478206),\n",
       " (186, 0.0),\n",
       " (187, 0.007257713505423644),\n",
       " (188, 0.0021474758285544475),\n",
       " (189, 0.0094835563724816),\n",
       " (190, 0.0),\n",
       " (191, 0.0005281472913058583),\n",
       " (192, 0.00022210866279850436),\n",
       " (193, 0.0),\n",
       " (194, 0.0),\n",
       " (195, 0.00029114754098360655),\n",
       " (196, 0.0),\n",
       " (197, 0.0),\n",
       " (198, 0.0005013066855172118),\n",
       " (199, 0.0),\n",
       " (200, 0.0),\n",
       " (201, 0.0003989683001913637),\n",
       " (202, 0.0),\n",
       " (203, 0.0),\n",
       " (204, 0.0),\n",
       " (205, 0.005406739441100212),\n",
       " (206, 0.0),\n",
       " (207, 0.0),\n",
       " (208, 0.000670957588629688),\n",
       " (209, 0.0),\n",
       " (210, 0.00629125926809838),\n",
       " (211, 0.0006049079959294666),\n",
       " (212, 0.0001624707259953162),\n",
       " (213, 0.00026234243697479006),\n",
       " (214, 0.0004757652220589955),\n",
       " (215, 0.0010047103396645732),\n",
       " (216, 0.010126054202524305),\n",
       " (217, 0.0),\n",
       " (218, 0.0),\n",
       " (219, 0.02075830652313379),\n",
       " (220, 0.0007238512343945001),\n",
       " (221, 0.01828171532006777),\n",
       " (222, 0.0003513849632560767),\n",
       " (223, 0.006075110700104354),\n",
       " (224, 0.012687638224456116),\n",
       " (225, 0.003565799945417684),\n",
       " (226, 0.0060456023872237025),\n",
       " (227, 0.00796822189780291),\n",
       " (228, 0.0006020822848266055),\n",
       " (229, 0.0004494467689617264),\n",
       " (230, 0.008551859914129673),\n",
       " (231, 0.0006954543264984982),\n",
       " (232, 0.0034859589269846734),\n",
       " (233, 0.0005500625856357076),\n",
       " (234, 0.003983632403749413),\n",
       " (235, 0.0),\n",
       " (236, 0.0),\n",
       " (237, 0.0003432966634429401),\n",
       " (238, 0.0),\n",
       " (239, 0.00879168097314257),\n",
       " (240, 0.004252504567262272),\n",
       " (241, 0.0032939318300282397),\n",
       " (242, 0.0002740903164631978),\n",
       " (243, 0.0024508998804412306),\n",
       " (244, 0.00018240710560625827),\n",
       " (245, 0.00019295958279009134),\n",
       " (246, 0.00571316171345852),\n",
       " (247, 0.00792904598705654),\n",
       " (248, 0.00141885021625496),\n",
       " (249, 0.0020446789653937895),\n",
       " (250, 0.006217138363245339),\n",
       " (251, 0.002210320749475543),\n",
       " (252, 0.0038791928319936737),\n",
       " (253, 0.0025260929954104405),\n",
       " (254, 0.00643046440737365),\n",
       " (255, 0.0020844328228741414),\n",
       " (256, 0.0),\n",
       " (257, 0.0),\n",
       " (258, 0.001646950375182433),\n",
       " (259, 0.0017634780044585832),\n",
       " (260, 0.0013573140549385768),\n",
       " (261, 0.004107572189960277),\n",
       " (262, 0.0009456182059719605),\n",
       " (263, 0.0013297109010777552),\n",
       " (264, 0.0032609033158302746),\n",
       " (265, 0.0018599985372931313),\n",
       " (266, 0.005811896554463553),\n",
       " (267, 0.0043598906822778175),\n",
       " (268, 0.004452675011985123),\n",
       " (269, 0.004070648300334878),\n",
       " (270, 0.0007886405104935635),\n",
       " (271, 0.001547552328027745),\n",
       " (272, 0.007387146008676763),\n",
       " (273, 0.00043017761641283984),\n",
       " (274, 0.0),\n",
       " (275, 0.0),\n",
       " (276, 0.0006812114517168366),\n",
       " (277, 0.0),\n",
       " (278, 0.0),\n",
       " (279, 0.006244699000203046),\n",
       " (280, 0.0002962418300653595),\n",
       " (281, 0.0),\n",
       " (282, 0.0),\n",
       " (283, 0.0),\n",
       " (284, 0.0),\n",
       " (285, 0.0009987913064539047),\n",
       " (286, 0.0),\n",
       " (287, 0.0002411656970480502),\n",
       " (288, 4.470984103830676e-05),\n",
       " (289, 0.0),\n",
       " (290, 0.00043787600298463475),\n",
       " (291, 0.0),\n",
       " (292, 0.00018137254901960786),\n",
       " (293, 0.0006652936086935264),\n",
       " (294, 0.0),\n",
       " (295, 0.0003286571918647398),\n",
       " (296, 0.00036290668945956287),\n",
       " (297, 0.0058917714515835185),\n",
       " (298, 0.004039227283696017),\n",
       " (299, 0.0),\n",
       " (300, 0.0),\n",
       " (301, 0.0),\n",
       " (302, 0.0006045640968457917),\n",
       " (303, 0.0),\n",
       " (304, 0.00016054752014413662),\n",
       " (305, 0.0003282663985331521),\n",
       " (306, 0.0028421561385921974),\n",
       " (307, 0.0),\n",
       " (308, 0.0005408261598386874),\n",
       " (309, 0.0008764063042483887),\n",
       " (310, 0.0),\n",
       " (311, 0.000581713635534629),\n",
       " (312, 0.0),\n",
       " (313, 0.00035629126925898735),\n",
       " (314, 0.004780625661777615),\n",
       " (315, 0.0033711262805135992),\n",
       " (316, 0.00580406121502218),\n",
       " (317, 0.0020268337667570173),\n",
       " (318, 0.0022266386636925683),\n",
       " (319, 0.005509121971384275),\n",
       " (320, 2.1356421356421495e-05),\n",
       " (321, 0.005792710411940778),\n",
       " (322, 0.0),\n",
       " (323, 0.0002191334527541422),\n",
       " (324, 0.0035933243497596194),\n",
       " (325, 0.0),\n",
       " (326, 0.007759643378358339),\n",
       " (327, 0.0065451671038114475),\n",
       " (328, 0.006623612919753093),\n",
       " (329, 0.0),\n",
       " (330, 1.8019480519480456e-05),\n",
       " (331, 0.0),\n",
       " (332, 0.0005911859973837063),\n",
       " (333, 0.0),\n",
       " (334, 0.005666353758522773),\n",
       " (335, 0.00538430486508768),\n",
       " (336, 0.012018485553775252),\n",
       " (337, 0.0),\n",
       " (338, 0.0005008367716598513),\n",
       " (339, 0.0009199251851977156),\n",
       " (340, 0.0011910770347942246),\n",
       " (341, 0.0006751771957613811),\n",
       " (342, 0.006427248283479443),\n",
       " (343, 0.0022033979017440594),\n",
       " (344, 0.001628265278848462),\n",
       " (345, 0.00047640556554541183),\n",
       " (346, 0.0005880175309018745),\n",
       " (347, 0.0034567736995368063),\n",
       " (348, 0.0),\n",
       " (349, 0.0003359564164648912),\n",
       " (350, 0.0),\n",
       " (351, 0.0),\n",
       " (352, 0.0),\n",
       " (353, 0.0008644566864491679),\n",
       " (354, 0.0008221401087002157),\n",
       " (355, 0.0),\n",
       " (356, 0.006212925079688377),\n",
       " (357, 0.0005504132231404958),\n",
       " (358, 0.008087863802914836),\n",
       " (359, 0.0),\n",
       " (360, 0.003781830466381763),\n",
       " (361, 0.0003785166240409205),\n",
       " (362, 0.0002587964039370688),\n",
       " (363, 0.0),\n",
       " (364, 0.0),\n",
       " (365, 0.0007788818865955541),\n",
       " (366, 0.0006904751106264145),\n",
       " (367, 0.00023945733521472267),\n",
       " (368, 0.0),\n",
       " (369, 0.0004333116460637606),\n",
       " (370, 0.00018031189083820664),\n",
       " (371, 0.0),\n",
       " (372, 0.0),\n",
       " (373, 0.0),\n",
       " (374, 9.2809364548495e-05),\n",
       " (375, 0.0007681367717658395),\n",
       " (376, 0.0007283730952434728),\n",
       " (377, 0.0),\n",
       " (378, 0.0),\n",
       " (379, 0.0),\n",
       " (380, 0.0018504434541789679),\n",
       " (381, 0.0005598484386396964),\n",
       " (382, 0.0),\n",
       " (383, 0.0010298744791267225),\n",
       " (384, 0.0004084871485250991),\n",
       " (385, 0.0),\n",
       " (386, 0.0),\n",
       " (387, 0.0),\n",
       " (388, 0.0),\n",
       " (389, 0.00044687189672294064),\n",
       " (390, 0.0),\n",
       " (391, 0.0),\n",
       " (392, 0.0004101809954751132),\n",
       " (393, 0.0),\n",
       " (394, 0.005457506433718643),\n",
       " (395, 0.0007351527210492299),\n",
       " (396, 0.0),\n",
       " (397, 0.0),\n",
       " (398, 0.0001420187793427226),\n",
       " (399, 0.0),\n",
       " (400, 0.003819625389521912),\n",
       " (401, 0.008707166244634357),\n",
       " (402, 0.000361802902979373),\n",
       " (403, 0.0),\n",
       " (404, 0.0),\n",
       " (405, 0.0),\n",
       " (406, 0.00890077154449755),\n",
       " (407, 0.012062676445712916),\n",
       " (408, 0.0022085681744805708),\n",
       " (409, 0.0007694929874248056),\n",
       " (410, 0.011216303120598958),\n",
       " (411, 0.0005912272416322566),\n",
       " (412, 0.00016818181818181795),\n",
       " (413, 0.00036458333333333167),\n",
       " (414, 0.0),\n",
       " (415, 0.0),\n",
       " (416, 0.0),\n",
       " (417, 0.0),\n",
       " (418, 0.0008934986139436402),\n",
       " (419, 0.000376874185136897),\n",
       " (420, 0.0),\n",
       " (421, 0.0058319020592049586),\n",
       " (422, 0.0),\n",
       " (423, 0.0),\n",
       " (424, 0.0003396721311475411),\n",
       " (425, 0.0),\n",
       " (426, 0.001821766788760763),\n",
       " (427, 0.00031567680408657497),\n",
       " (428, 0.0),\n",
       " (429, 0.0011201611756902932),\n",
       " (430, 0.003997400951860724),\n",
       " (431, 0.0),\n",
       " (432, 0.0),\n",
       " (433, 0.001083325880901547),\n",
       " (434, 0.003512307971090725),\n",
       " (435, 0.0072769725809012295),\n",
       " (436, 0.0),\n",
       " (437, 0.0),\n",
       " (438, 0.0),\n",
       " (439, 0.0066276350217241665),\n",
       " (440, 0.0006348876577408433),\n",
       " (441, 0.0),\n",
       " (442, 0.0002393011216566002),\n",
       " (443, 0.0),\n",
       " (444, 0.006405116499245586),\n",
       " (445, 0.01035367946921208),\n",
       " (446, 0.0),\n",
       " (447, 0.0),\n",
       " (448, 0.0),\n",
       " (449, 0.0004986192630635038),\n",
       " (450, 0.0),\n",
       " (451, 0.0),\n",
       " (452, 0.0002702918649910399),\n",
       " (453, 0.0005543706761457019),\n",
       " (454, 0.0008167080826545271),\n",
       " (455, 0.00019337979094076658),\n",
       " (456, 0.0002212888637648387),\n",
       " (457, 0.00034268437368441203),\n",
       " (458, 0.008422388240699531),\n",
       " (459, 0.0002308377896613189),\n",
       " (460, 0.0002653644314868807),\n",
       " (461, 0.0),\n",
       " (462, 0.0),\n",
       " (463, 0.0002562704918032786),\n",
       " (464, 0.0),\n",
       " (465, 0.0),\n",
       " (466, 0.0010275888129964224),\n",
       " (467, 0.0),\n",
       " (468, 0.0),\n",
       " (469, 0.0),\n",
       " (470, 0.0),\n",
       " (471, 0.0),\n",
       " (472, 0.0),\n",
       " (473, 0.0012127077407312885),\n",
       " (474, 0.0023855145731489333),\n",
       " (475, 0.0025551110640900654),\n",
       " (476, 0.001031308168047452),\n",
       " (477, 0.0),\n",
       " (478, 0.0),\n",
       " (479, 0.0),\n",
       " (480, 0.006431843014766007),\n",
       " (481, 0.0004930056652504773),\n",
       " (482, 0.0),\n",
       " (483, 0.01149843369730846),\n",
       " (484, 0.0),\n",
       " (485, 0.0),\n",
       " (486, 0.011636235265138623),\n",
       " (487, 0.0),\n",
       " (488, 0.0),\n",
       " (489, 0.0),\n",
       " (490, 0.0),\n",
       " (491, 0.0),\n",
       " (492, 0.009787718749128791),\n",
       " (493, 0.0),\n",
       " (494, 0.005038161031655227),\n",
       " (495, 0.0),\n",
       " (496, 0.0),\n",
       " (497, 0.0),\n",
       " (498, 0.00020545834332089225),\n",
       " (499, 0.000684035571233353),\n",
       " (500, 0.00010853976531942636),\n",
       " (501, 0.0002529049897470949),\n",
       " (502, 3.0051981806367788e-05),\n",
       " (503, 0.0),\n",
       " (504, 0.0),\n",
       " (505, 0.0),\n",
       " (506, 0.0070197240561365145),\n",
       " (507, 0.008409639719447843),\n",
       " (508, 0.0),\n",
       " (509, 0.0),\n",
       " (510, 0.0),\n",
       " (511, 0.0),\n",
       " (512, 0.0),\n",
       " (513, 0.0002853287063813383),\n",
       " (514, 0.0),\n",
       " (515, 0.0005093302667316965),\n",
       " (516, 0.0003377861964457906),\n",
       " (517, 0.008081305160643211),\n",
       " (518, 0.0002543973949732049),\n",
       " (519, 0.0),\n",
       " (520, 0.0005939799331103678),\n",
       " (521, 0.0),\n",
       " (522, 0.0),\n",
       " (523, 0.0),\n",
       " (524, 0.00029721311475409837),\n",
       " (525, 0.0001865546218487395),\n",
       " (526, 0.0005070293863878601),\n",
       " (527, 0.00012300531914893623),\n",
       " (528, 0.0),\n",
       " (529, 0.0),\n",
       " (530, 0.0),\n",
       " (531, 0.0),\n",
       " (532, 0.0002767619680851065),\n",
       " (533, 0.0004154078757265626),\n",
       " (534, 0.000465739070634527),\n",
       " (535, 0.000323438353669641),\n",
       " (536, 0.0),\n",
       " (537, 0.0010461968556285237),\n",
       " (538, 0.0),\n",
       " (539, 0.0),\n",
       " (540, 0.0005202033405954976),\n",
       " (541, 0.0006230659549732632),\n",
       " (542, 0.0),\n",
       " (543, 0.0),\n",
       " (544, 0.0),\n",
       " (545, 0.0015918434687263673),\n",
       " (546, 0.0029160122156920037),\n",
       " (547, 0.005551294783515404),\n",
       " (548, 0.0011410342705433182),\n",
       " (549, 0.0004913059066588066),\n",
       " (550, 0.0048678682894586715),\n",
       " (551, 0.003145640374580453),\n",
       " (552, 0.0010855954715433508),\n",
       " (553, 0.004831739634437583),\n",
       " (554, 0.0),\n",
       " (555, 0.0002952127659574468),\n",
       " (556, 0.0),\n",
       " (557, 0.003900550304272107),\n",
       " (558, 0.0002933016983016985),\n",
       " (559, 0.0),\n",
       " (560, 0.0),\n",
       " (561, 0.0),\n",
       " (562, 0.0),\n",
       " (563, 0.0),\n",
       " (564, 0.008400728564565885),\n",
       " (565, 0.0),\n",
       " (566, 0.0),\n",
       " (567, 0.0),\n",
       " (568, 0.0),\n",
       " (569, 0.0),\n",
       " (570, 0.0),\n",
       " (571, 0.0),\n",
       " (572, 0.0),\n",
       " (573, 0.0),\n",
       " (574, 0.003893520935962332),\n",
       " (575, 0.005266221107783551),\n",
       " (576, 0.0),\n",
       " (577, 0.0),\n",
       " (578, 0.0),\n",
       " (579, 0.0005887945051515688),\n",
       " (580, 0.0),\n",
       " (581, 0.0),\n",
       " (582, 0.0023938263269717907),\n",
       " (583, 0.00041511166678653074),\n",
       " (584, 0.0),\n",
       " (585, 0.0),\n",
       " (586, 0.0),\n",
       " (587, 0.0),\n",
       " (588, 0.0),\n",
       " (589, 0.0),\n",
       " (590, 0.0),\n",
       " (591, 0.0),\n",
       " (592, 0.0),\n",
       " (593, 0.0006584280771131984),\n",
       " (594, 0.0),\n",
       " (595, 0.0006253957906500275),\n",
       " (596, 0.0),\n",
       " (597, 0.0),\n",
       " (598, 0.0),\n",
       " (599, 0.0004190233729404239),\n",
       " (600, 0.0),\n",
       " (601, 0.0013375994788986747),\n",
       " (602, 0.0),\n",
       " (603, 0.0002411994784876141),\n",
       " (604, 0.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = rf.feature_importances_\n",
    "features_index = np.arange(0, X_train.shape[1], 1, dtype=int)\n",
    "f_i = list(zip(features_index, feature_importance))\n",
    "f_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7be1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(219, 0.02075830652313379),\n",
       " (221, 0.01828171532006777),\n",
       " (132, 0.017624535977725196),\n",
       " (110, 0.01563127582115065),\n",
       " (114, 0.014698056176738996),\n",
       " (224, 0.012687638224456116),\n",
       " (153, 0.012255683808618911),\n",
       " (109, 0.012225196054643263),\n",
       " (407, 0.012062676445712916),\n",
       " (336, 0.012018485553775252),\n",
       " (41, 0.011665518870973637),\n",
       " (486, 0.011636235265138623),\n",
       " (483, 0.01149843369730846),\n",
       " (410, 0.011216303120598958),\n",
       " (149, 0.011177275014258336),\n",
       " (445, 0.01035367946921208),\n",
       " (216, 0.010126054202524305),\n",
       " (492, 0.009787718749128791),\n",
       " (115, 0.009763416806037931),\n",
       " (189, 0.0094835563724816),\n",
       " (30, 0.009011324992868105),\n",
       " (406, 0.00890077154449755),\n",
       " (15, 0.00889783928726194),\n",
       " (239, 0.00879168097314257),\n",
       " (401, 0.008707166244634357),\n",
       " (230, 0.008551859914129673),\n",
       " (458, 0.008422388240699531),\n",
       " (507, 0.008409639719447843),\n",
       " (564, 0.008400728564565885),\n",
       " (158, 0.008351604565017592),\n",
       " (358, 0.008087863802914836),\n",
       " (517, 0.008081305160643211),\n",
       " (157, 0.008044715496184891),\n",
       " (227, 0.00796822189780291),\n",
       " (247, 0.00792904598705654),\n",
       " (151, 0.007917767880397894),\n",
       " (34, 0.00789860705029066),\n",
       " (54, 0.007870034377194194),\n",
       " (326, 0.007759643378358339),\n",
       " (272, 0.007387146008676763),\n",
       " (435, 0.0072769725809012295),\n",
       " (187, 0.007257713505423644),\n",
       " (155, 0.007135762015509411),\n",
       " (506, 0.0070197240561365145),\n",
       " (24, 0.0068979842755490395),\n",
       " (176, 0.006814339385125424),\n",
       " (129, 0.0067780267968286),\n",
       " (439, 0.0066276350217241665),\n",
       " (328, 0.006623612919753093),\n",
       " (327, 0.0065451671038114475),\n",
       " (480, 0.006431843014766007),\n",
       " (254, 0.00643046440737365),\n",
       " (342, 0.006427248283479443),\n",
       " (444, 0.006405116499245586),\n",
       " (38, 0.006391928747776974),\n",
       " (210, 0.00629125926809838),\n",
       " (62, 0.006247625553345409),\n",
       " (279, 0.006244699000203046),\n",
       " (250, 0.006217138363245339),\n",
       " (356, 0.006212925079688377),\n",
       " (172, 0.006085179024998635),\n",
       " (223, 0.006075110700104354),\n",
       " (226, 0.0060456023872237025),\n",
       " (122, 0.006003320754618446),\n",
       " (116, 0.005903016603124396),\n",
       " (297, 0.0058917714515835185),\n",
       " (421, 0.0058319020592049586),\n",
       " (266, 0.005811896554463553),\n",
       " (316, 0.00580406121502218),\n",
       " (321, 0.005792710411940778),\n",
       " (118, 0.005753054650508807),\n",
       " (246, 0.00571316171345852),\n",
       " (46, 0.005700543569232025),\n",
       " (334, 0.005666353758522773),\n",
       " (53, 0.005605431644658461),\n",
       " (547, 0.005551294783515404),\n",
       " (319, 0.005509121971384275),\n",
       " (394, 0.005457506433718643),\n",
       " (161, 0.005432979833776292),\n",
       " (205, 0.005406739441100212),\n",
       " (335, 0.00538430486508768),\n",
       " (154, 0.005304996729736281),\n",
       " (124, 0.005296323944201637),\n",
       " (575, 0.005266221107783551),\n",
       " (125, 0.005065462672593194),\n",
       " (494, 0.005038161031655227),\n",
       " (550, 0.0048678682894586715),\n",
       " (553, 0.004831739634437583),\n",
       " (314, 0.004780625661777615),\n",
       " (23, 0.004581500797855939),\n",
       " (7, 0.004526001999516245),\n",
       " (17, 0.004505636562028702),\n",
       " (26, 0.00449119878325455),\n",
       " (268, 0.004452675011985123),\n",
       " (72, 0.00442702059880124),\n",
       " (10, 0.004399380603239685),\n",
       " (267, 0.0043598906822778175),\n",
       " (240, 0.004252504567262272),\n",
       " (31, 0.004146511897527134),\n",
       " (261, 0.004107572189960277),\n",
       " (269, 0.004070648300334878),\n",
       " (298, 0.004039227283696017),\n",
       " (430, 0.003997400951860724),\n",
       " (234, 0.003983632403749413),\n",
       " (557, 0.003900550304272107),\n",
       " (574, 0.003893520935962332),\n",
       " (252, 0.0038791928319936737),\n",
       " (12, 0.0038685054970028247),\n",
       " (400, 0.003819625389521912),\n",
       " (360, 0.003781830466381763),\n",
       " (324, 0.0035933243497596194),\n",
       " (225, 0.003565799945417684),\n",
       " (434, 0.003512307971090725),\n",
       " (232, 0.0034859589269846734),\n",
       " (347, 0.0034567736995368063),\n",
       " (315, 0.0033711262805135992),\n",
       " (78, 0.0033411648462032787),\n",
       " (241, 0.0032939318300282397),\n",
       " (264, 0.0032609033158302746),\n",
       " (103, 0.0032457538000745583),\n",
       " (551, 0.003145640374580453),\n",
       " (546, 0.0029160122156920037),\n",
       " (112, 0.002845025589048266),\n",
       " (306, 0.0028421561385921974),\n",
       " (142, 0.002721877775872547),\n",
       " (16, 0.0026729969656460882),\n",
       " (475, 0.0025551110640900654),\n",
       " (152, 0.002550275912422965),\n",
       " (253, 0.0025260929954104405),\n",
       " (243, 0.0024508998804412306),\n",
       " (582, 0.0023938263269717907),\n",
       " (474, 0.0023855145731489333),\n",
       " (318, 0.0022266386636925683),\n",
       " (251, 0.002210320749475543),\n",
       " (408, 0.0022085681744805708),\n",
       " (343, 0.0022033979017440594),\n",
       " (188, 0.0021474758285544475),\n",
       " (255, 0.0020844328228741414),\n",
       " (19, 0.002060010009764487),\n",
       " (249, 0.0020446789653937895),\n",
       " (11, 0.00204334628154042),\n",
       " (317, 0.0020268337667570173),\n",
       " (14, 0.0019266384822382418),\n",
       " (265, 0.0018599985372931313),\n",
       " (8, 0.001857925037690587),\n",
       " (380, 0.0018504434541789679),\n",
       " (426, 0.001821766788760763),\n",
       " (259, 0.0017634780044585832),\n",
       " (258, 0.001646950375182433),\n",
       " (344, 0.001628265278848462),\n",
       " (545, 0.0015918434687263673),\n",
       " (271, 0.001547552328027745),\n",
       " (81, 0.001473773036904195),\n",
       " (248, 0.00141885021625496),\n",
       " (77, 0.0013780996089357645),\n",
       " (260, 0.0013573140549385768),\n",
       " (601, 0.0013375994788986747),\n",
       " (263, 0.0013297109010777552),\n",
       " (473, 0.0012127077407312885),\n",
       " (340, 0.0011910770347942246),\n",
       " (64, 0.0011732724996365886),\n",
       " (136, 0.0011475646780163917),\n",
       " (13, 0.001143099004319998),\n",
       " (548, 0.0011410342705433182),\n",
       " (429, 0.0011201611756902932),\n",
       " (33, 0.001109496564669569),\n",
       " (552, 0.0010855954715433508),\n",
       " (433, 0.001083325880901547),\n",
       " (537, 0.0010461968556285237),\n",
       " (476, 0.001031308168047452),\n",
       " (383, 0.0010298744791267225),\n",
       " (466, 0.0010275888129964224),\n",
       " (35, 0.0010263924256141389),\n",
       " (215, 0.0010047103396645732),\n",
       " (285, 0.0009987913064539047),\n",
       " (262, 0.0009456182059719605),\n",
       " (339, 0.0009199251851977156),\n",
       " (418, 0.0008934986139436402),\n",
       " (309, 0.0008764063042483887),\n",
       " (353, 0.0008644566864491679),\n",
       " (354, 0.0008221401087002157),\n",
       " (454, 0.0008167080826545271),\n",
       " (135, 0.0008005833089400542),\n",
       " (270, 0.0007886405104935635),\n",
       " (68, 0.0007826455132993188),\n",
       " (365, 0.0007788818865955541),\n",
       " (106, 0.0007705388973771326),\n",
       " (409, 0.0007694929874248056),\n",
       " (375, 0.0007681367717658395),\n",
       " (82, 0.000736197580677173),\n",
       " (395, 0.0007351527210492299),\n",
       " (376, 0.0007283730952434728),\n",
       " (220, 0.0007238512343945001),\n",
       " (231, 0.0006954543264984982),\n",
       " (366, 0.0006904751106264145),\n",
       " (499, 0.000684035571233353),\n",
       " (276, 0.0006812114517168366),\n",
       " (341, 0.0006751771957613811),\n",
       " (208, 0.000670957588629688),\n",
       " (89, 0.0006680913624190935),\n",
       " (293, 0.0006652936086935264),\n",
       " (593, 0.0006584280771131984),\n",
       " (90, 0.0006509289897114438),\n",
       " (143, 0.0006384692077613314),\n",
       " (440, 0.0006348876577408433),\n",
       " (595, 0.0006253957906500275),\n",
       " (541, 0.0006230659549732632),\n",
       " (211, 0.0006049079959294666),\n",
       " (302, 0.0006045640968457917),\n",
       " (228, 0.0006020822848266055),\n",
       " (520, 0.0005939799331103678),\n",
       " (411, 0.0005912272416322566),\n",
       " (332, 0.0005911859973837063),\n",
       " (579, 0.0005887945051515688),\n",
       " (346, 0.0005880175309018745),\n",
       " (311, 0.000581713635534629),\n",
       " (134, 0.0005784387265878148),\n",
       " (381, 0.0005598484386396964),\n",
       " (453, 0.0005543706761457019),\n",
       " (357, 0.0005504132231404958),\n",
       " (233, 0.0005500625856357076),\n",
       " (173, 0.0005428786023192623),\n",
       " (308, 0.0005408261598386874),\n",
       " (191, 0.0005281472913058583),\n",
       " (540, 0.0005202033405954976),\n",
       " (156, 0.0005201459243424113),\n",
       " (515, 0.0005093302667316965),\n",
       " (526, 0.0005070293863878601),\n",
       " (198, 0.0005013066855172118),\n",
       " (338, 0.0005008367716598513),\n",
       " (449, 0.0004986192630635038),\n",
       " (481, 0.0004930056652504773),\n",
       " (549, 0.0004913059066588066),\n",
       " (147, 0.0004802450984673645),\n",
       " (345, 0.00047640556554541183),\n",
       " (214, 0.0004757652220589955),\n",
       " (96, 0.0004718282368249835),\n",
       " (141, 0.0004690468429875218),\n",
       " (534, 0.000465739070634527),\n",
       " (229, 0.0004494467689617264),\n",
       " (389, 0.00044687189672294064),\n",
       " (160, 0.0004464555052790345),\n",
       " (290, 0.00043787600298463475),\n",
       " (369, 0.0004333116460637606),\n",
       " (273, 0.00043017761641283984),\n",
       " (42, 0.0004221033868092693),\n",
       " (599, 0.0004190233729404239),\n",
       " (533, 0.0004154078757265626),\n",
       " (583, 0.00041511166678653074),\n",
       " (392, 0.0004101809954751132),\n",
       " (384, 0.0004084871485250991),\n",
       " (56, 0.00040284733966805075),\n",
       " (201, 0.0003989683001913637),\n",
       " (361, 0.0003785166240409205),\n",
       " (419, 0.000376874185136897),\n",
       " (5, 0.00037525354969574165),\n",
       " (413, 0.00036458333333333167),\n",
       " (9, 0.0003629978684550598),\n",
       " (296, 0.00036290668945956287),\n",
       " (402, 0.000361802902979373),\n",
       " (313, 0.00035629126925898735),\n",
       " (222, 0.0003513849632560767),\n",
       " (237, 0.0003432966634429401),\n",
       " (457, 0.00034268437368441203),\n",
       " (424, 0.0003396721311475411),\n",
       " (516, 0.0003377861964457906),\n",
       " (349, 0.0003359564164648912),\n",
       " (102, 0.0003295147020556849),\n",
       " (295, 0.0003286571918647398),\n",
       " (305, 0.0003282663985331521),\n",
       " (164, 0.0003276269185360095),\n",
       " (185, 0.0003249837345478206),\n",
       " (535, 0.000323438353669641),\n",
       " (69, 0.0003207249802994489),\n",
       " (427, 0.00031567680408657497),\n",
       " (131, 0.00030228758169934645),\n",
       " (55, 0.00029861286254728904),\n",
       " (524, 0.00029721311475409837),\n",
       " (280, 0.0002962418300653595),\n",
       " (555, 0.0002952127659574468),\n",
       " (27, 0.00029365079365079364),\n",
       " (558, 0.0002933016983016985),\n",
       " (195, 0.00029114754098360655),\n",
       " (513, 0.0002853287063813383),\n",
       " (108, 0.000279206510190283),\n",
       " (532, 0.0002767619680851065),\n",
       " (242, 0.0002740903164631978),\n",
       " (166, 0.0002729508196721312),\n",
       " (65, 0.00027281746031746206),\n",
       " (452, 0.0002702918649910399),\n",
       " (70, 0.0002688143665723838),\n",
       " (460, 0.0002653644314868807),\n",
       " (213, 0.00026234243697479006),\n",
       " (182, 0.0002587964039370688),\n",
       " (362, 0.0002587964039370688),\n",
       " (463, 0.0002562704918032786),\n",
       " (518, 0.0002543973949732049),\n",
       " (501, 0.0002529049897470949),\n",
       " (97, 0.00024378030782205355),\n",
       " (603, 0.0002411994784876141),\n",
       " (287, 0.0002411656970480502),\n",
       " (117, 0.00024109599326400708),\n",
       " (3, 0.0002404158544509422),\n",
       " (367, 0.00023945733521472267),\n",
       " (442, 0.0002393011216566002),\n",
       " (162, 0.00023499624815579738),\n",
       " (95, 0.00023223363419441873),\n",
       " (459, 0.0002308377896613189),\n",
       " (192, 0.00022210866279850436),\n",
       " (456, 0.0002212888637648387),\n",
       " (323, 0.0002191334527541422),\n",
       " (120, 0.000214175456650373),\n",
       " (498, 0.00020545834332089225),\n",
       " (43, 0.0001989247311827958),\n",
       " (455, 0.00019337979094076658),\n",
       " (245, 0.00019295958279009134),\n",
       " (83, 0.00018743667679837903),\n",
       " (525, 0.0001865546218487395),\n",
       " (244, 0.00018240710560625827),\n",
       " (292, 0.00018137254901960786),\n",
       " (370, 0.00018031189083820664),\n",
       " (44, 0.00017583124754423404),\n",
       " (412, 0.00016818181818181795),\n",
       " (212, 0.0001624707259953162),\n",
       " (304, 0.00016054752014413662),\n",
       " (398, 0.0001420187793427226),\n",
       " (527, 0.00012300531914893623),\n",
       " (184, 0.00012059973924380707),\n",
       " (500, 0.00010853976531942636),\n",
       " (374, 9.2809364548495e-05),\n",
       " (1, 8.47975553857907e-05),\n",
       " (288, 4.470984103830676e-05),\n",
       " (91, 3.032786885245903e-05),\n",
       " (502, 3.0051981806367788e-05),\n",
       " (320, 2.1356421356421495e-05),\n",
       " (330, 1.8019480519480456e-05),\n",
       " (0, 0.0),\n",
       " (2, 0.0),\n",
       " (4, 0.0),\n",
       " (6, 0.0),\n",
       " (18, 0.0),\n",
       " (20, 0.0),\n",
       " (21, 0.0),\n",
       " (22, 0.0),\n",
       " (25, 0.0),\n",
       " (28, 0.0),\n",
       " (29, 0.0),\n",
       " (32, 0.0),\n",
       " (36, 0.0),\n",
       " (37, 0.0),\n",
       " (39, 0.0),\n",
       " (40, 0.0),\n",
       " (45, 0.0),\n",
       " (47, 0.0),\n",
       " (48, 0.0),\n",
       " (49, 0.0),\n",
       " (50, 0.0),\n",
       " (51, 0.0),\n",
       " (52, 0.0),\n",
       " (57, 0.0),\n",
       " (58, 0.0),\n",
       " (59, 0.0),\n",
       " (60, 0.0),\n",
       " (61, 0.0),\n",
       " (63, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (71, 0.0),\n",
       " (73, 0.0),\n",
       " (74, 0.0),\n",
       " (75, 0.0),\n",
       " (76, 0.0),\n",
       " (79, 0.0),\n",
       " (80, 0.0),\n",
       " (84, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (87, 0.0),\n",
       " (88, 0.0),\n",
       " (92, 0.0),\n",
       " (93, 0.0),\n",
       " (94, 0.0),\n",
       " (98, 0.0),\n",
       " (99, 0.0),\n",
       " (100, 0.0),\n",
       " (101, 0.0),\n",
       " (104, 0.0),\n",
       " (105, 0.0),\n",
       " (107, 0.0),\n",
       " (111, 0.0),\n",
       " (113, 0.0),\n",
       " (119, 0.0),\n",
       " (121, 0.0),\n",
       " (123, 0.0),\n",
       " (126, 0.0),\n",
       " (127, 0.0),\n",
       " (128, 0.0),\n",
       " (130, 0.0),\n",
       " (133, 0.0),\n",
       " (137, 0.0),\n",
       " (138, 0.0),\n",
       " (139, 0.0),\n",
       " (140, 0.0),\n",
       " (144, 0.0),\n",
       " (145, 0.0),\n",
       " (146, 0.0),\n",
       " (148, 0.0),\n",
       " (150, 0.0),\n",
       " (159, 0.0),\n",
       " (163, 0.0),\n",
       " (165, 0.0),\n",
       " (167, 0.0),\n",
       " (168, 0.0),\n",
       " (169, 0.0),\n",
       " (170, 0.0),\n",
       " (171, 0.0),\n",
       " (174, 0.0),\n",
       " (175, 0.0),\n",
       " (177, 0.0),\n",
       " (178, 0.0),\n",
       " (179, 0.0),\n",
       " (180, 0.0),\n",
       " (181, 0.0),\n",
       " (183, 0.0),\n",
       " (186, 0.0),\n",
       " (190, 0.0),\n",
       " (193, 0.0),\n",
       " (194, 0.0),\n",
       " (196, 0.0),\n",
       " (197, 0.0),\n",
       " (199, 0.0),\n",
       " (200, 0.0),\n",
       " (202, 0.0),\n",
       " (203, 0.0),\n",
       " (204, 0.0),\n",
       " (206, 0.0),\n",
       " (207, 0.0),\n",
       " (209, 0.0),\n",
       " (217, 0.0),\n",
       " (218, 0.0),\n",
       " (235, 0.0),\n",
       " (236, 0.0),\n",
       " (238, 0.0),\n",
       " (256, 0.0),\n",
       " (257, 0.0),\n",
       " (274, 0.0),\n",
       " (275, 0.0),\n",
       " (277, 0.0),\n",
       " (278, 0.0),\n",
       " (281, 0.0),\n",
       " (282, 0.0),\n",
       " (283, 0.0),\n",
       " (284, 0.0),\n",
       " (286, 0.0),\n",
       " (289, 0.0),\n",
       " (291, 0.0),\n",
       " (294, 0.0),\n",
       " (299, 0.0),\n",
       " (300, 0.0),\n",
       " (301, 0.0),\n",
       " (303, 0.0),\n",
       " (307, 0.0),\n",
       " (310, 0.0),\n",
       " (312, 0.0),\n",
       " (322, 0.0),\n",
       " (325, 0.0),\n",
       " (329, 0.0),\n",
       " (331, 0.0),\n",
       " (333, 0.0),\n",
       " (337, 0.0),\n",
       " (348, 0.0),\n",
       " (350, 0.0),\n",
       " (351, 0.0),\n",
       " (352, 0.0),\n",
       " (355, 0.0),\n",
       " (359, 0.0),\n",
       " (363, 0.0),\n",
       " (364, 0.0),\n",
       " (368, 0.0),\n",
       " (371, 0.0),\n",
       " (372, 0.0),\n",
       " (373, 0.0),\n",
       " (377, 0.0),\n",
       " (378, 0.0),\n",
       " (379, 0.0),\n",
       " (382, 0.0),\n",
       " (385, 0.0),\n",
       " (386, 0.0),\n",
       " (387, 0.0),\n",
       " (388, 0.0),\n",
       " (390, 0.0),\n",
       " (391, 0.0),\n",
       " (393, 0.0),\n",
       " (396, 0.0),\n",
       " (397, 0.0),\n",
       " (399, 0.0),\n",
       " (403, 0.0),\n",
       " (404, 0.0),\n",
       " (405, 0.0),\n",
       " (414, 0.0),\n",
       " (415, 0.0),\n",
       " (416, 0.0),\n",
       " (417, 0.0),\n",
       " (420, 0.0),\n",
       " (422, 0.0),\n",
       " (423, 0.0),\n",
       " (425, 0.0),\n",
       " (428, 0.0),\n",
       " (431, 0.0),\n",
       " (432, 0.0),\n",
       " (436, 0.0),\n",
       " (437, 0.0),\n",
       " (438, 0.0),\n",
       " (441, 0.0),\n",
       " (443, 0.0),\n",
       " (446, 0.0),\n",
       " (447, 0.0),\n",
       " (448, 0.0),\n",
       " (450, 0.0),\n",
       " (451, 0.0),\n",
       " (461, 0.0),\n",
       " (462, 0.0),\n",
       " (464, 0.0),\n",
       " (465, 0.0),\n",
       " (467, 0.0),\n",
       " (468, 0.0),\n",
       " (469, 0.0),\n",
       " (470, 0.0),\n",
       " (471, 0.0),\n",
       " (472, 0.0),\n",
       " (477, 0.0),\n",
       " (478, 0.0),\n",
       " (479, 0.0),\n",
       " (482, 0.0),\n",
       " (484, 0.0),\n",
       " (485, 0.0),\n",
       " (487, 0.0),\n",
       " (488, 0.0),\n",
       " (489, 0.0),\n",
       " (490, 0.0),\n",
       " (491, 0.0),\n",
       " (493, 0.0),\n",
       " (495, 0.0),\n",
       " (496, 0.0),\n",
       " (497, 0.0),\n",
       " (503, 0.0),\n",
       " (504, 0.0),\n",
       " (505, 0.0),\n",
       " (508, 0.0),\n",
       " (509, 0.0),\n",
       " (510, 0.0),\n",
       " (511, 0.0),\n",
       " (512, 0.0),\n",
       " (514, 0.0),\n",
       " (519, 0.0),\n",
       " (521, 0.0),\n",
       " (522, 0.0),\n",
       " (523, 0.0),\n",
       " (528, 0.0),\n",
       " (529, 0.0),\n",
       " (530, 0.0),\n",
       " (531, 0.0),\n",
       " (536, 0.0),\n",
       " (538, 0.0),\n",
       " (539, 0.0),\n",
       " (542, 0.0),\n",
       " (543, 0.0),\n",
       " (544, 0.0),\n",
       " (554, 0.0),\n",
       " (556, 0.0),\n",
       " (559, 0.0),\n",
       " (560, 0.0),\n",
       " (561, 0.0),\n",
       " (562, 0.0),\n",
       " (563, 0.0),\n",
       " (565, 0.0),\n",
       " (566, 0.0),\n",
       " (567, 0.0),\n",
       " (568, 0.0),\n",
       " (569, 0.0),\n",
       " (570, 0.0),\n",
       " (571, 0.0),\n",
       " (572, 0.0),\n",
       " (573, 0.0),\n",
       " (576, 0.0),\n",
       " (577, 0.0),\n",
       " (578, 0.0),\n",
       " (580, 0.0),\n",
       " (581, 0.0),\n",
       " (584, 0.0),\n",
       " (585, 0.0),\n",
       " (586, 0.0),\n",
       " (587, 0.0),\n",
       " (588, 0.0),\n",
       " (589, 0.0),\n",
       " (590, 0.0),\n",
       " (591, 0.0),\n",
       " (592, 0.0),\n",
       " (594, 0.0),\n",
       " (596, 0.0),\n",
       " (597, 0.0),\n",
       " (598, 0.0),\n",
       " (600, 0.0),\n",
       " (602, 0.0),\n",
       " (604, 0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_i_sorted = sorted(f_i,key=lambda col: col[1], reverse=True)\n",
    "f_i_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34399d39",
   "metadata": {},
   "source": [
    "## Recursive feature elemination - Random forest  <a name=\"Recursive_feature_elemination_Random_forest\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d721820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFE_RF(X_train, y_train, X_test, y_test, n_feature):\n",
    "    \n",
    "    # Apply scale datasets\n",
    "    # create model scale ==> standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    # fit dataset to model scale\n",
    "    X_train_scl = scaler.fit_transform(X_train)\n",
    "    y_train = y_train.ravel()\n",
    "    y_test = y_test.ravel()\n",
    "    \n",
    "    # Random forest classifier estimator\n",
    "    rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "    \n",
    "    # RFE model\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_feature, step=1, verbose=1)\n",
    "    rfe.fit_transform(X_train_scl, y_train)\n",
    "    \n",
    "    # Reduce X to the selected features.\n",
    "    X_train_reduce = rfe.transform(X_train)\n",
    "    X_test_reduce = rfe.transform(X_test)\n",
    "    \n",
    "    rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion = random_forest_classifier(X_train_reduce, y_train, X_test_reduce, y_test)\n",
    "    return rfe, rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a624441f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "classes model: \n",
      "[1 2]\n",
      "[[0.59 0.41]\n",
      " [0.59 0.41]\n",
      " [0.54 0.46]\n",
      " [0.46 0.54]\n",
      " [0.54 0.46]\n",
      " [0.5  0.5 ]\n",
      " [0.42 0.58]\n",
      " [0.63 0.37]\n",
      " [0.61 0.39]\n",
      " [0.55 0.45]\n",
      " [0.52 0.48]\n",
      " [0.66 0.34]\n",
      " [0.46 0.54]\n",
      " [0.55 0.45]\n",
      " [0.55 0.45]\n",
      " [0.5  0.5 ]\n",
      " [0.53 0.47]\n",
      " [0.49 0.51]\n",
      " [0.48 0.52]\n",
      " [0.38 0.62]\n",
      " [0.57 0.43]\n",
      " [0.62 0.38]\n",
      " [0.54 0.46]\n",
      " [0.46 0.54]\n",
      " [0.6  0.4 ]\n",
      " [0.49 0.51]\n",
      " [0.59 0.41]\n",
      " [0.56 0.44]]\n"
     ]
    }
   ],
   "source": [
    "rfe, rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion = RFE_RF(X_train, y_train, X_test, y_test, 390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5b77ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score = 0.6328125\n"
     ]
    }
   ],
   "source": [
    "print(\"auc_score = \"+ str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a25dc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4],\n",
       "       [12,  4]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf7e94e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  4, 12,  4], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d2ae291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4],\n",
       "       [12,  4]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b17410b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,\n",
       "        20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,\n",
       "        33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,\n",
       "        46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "        59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
       "        72,  73,  74,  75,  76,  77,  78,  80,  81,  82,  83,  84,  85,\n",
       "        86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "        99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127,\n",
       "       128, 129, 130, 132, 135, 136, 137, 139, 140, 141, 142, 145, 149,\n",
       "       151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 166, 169,\n",
       "       172, 173, 174, 176, 178, 181, 187, 188, 189, 190, 191, 195, 201,\n",
       "       205, 210, 212, 214, 216, 218, 219, 220, 221, 222, 223, 224, 225,\n",
       "       226, 227, 228, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
       "       241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
       "       255, 258, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270,\n",
       "       272, 279, 297, 298, 306, 307, 308, 309, 310, 311, 312, 313, 314,\n",
       "       315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n",
       "       328, 330, 331, 332, 334, 335, 336, 337, 339, 340, 341, 342, 343,\n",
       "       344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 356, 357, 358,\n",
       "       359, 360, 361, 362, 363, 364, 365, 367, 369, 370, 372, 374, 376,\n",
       "       378, 379, 380, 382, 383, 385, 387, 388, 391, 392, 393, 394, 398,\n",
       "       400, 401, 404, 406, 407, 408, 409, 410, 413, 414, 415, 417, 419,\n",
       "       421, 423, 426, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "       438, 439, 442, 444, 445, 449, 451, 455, 456, 457, 458, 459, 460,\n",
       "       463, 465, 469, 470, 471, 473, 474, 475, 476, 480, 481, 483, 485,\n",
       "       486, 488, 490, 491, 492, 494, 495, 499, 500, 505, 506, 507, 509,\n",
       "       510, 511, 512, 513, 516, 517, 526, 531, 532, 537, 540, 541, 542,\n",
       "       546, 547, 548, 550, 551, 552, 553, 554, 555, 557, 558, 560, 561,\n",
       "       562, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.arange(0, X_train.shape[1], 1, dtype=int)\n",
    "selected_features = np.array(features)[rfe.get_support()]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9aa7cc",
   "metadata": {},
   "source": [
    "Find number feature to select is the optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa1dde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_number_feature_to_select(min_feature, max_feature, sequence=50, savelink = \"./output/n_feature_to_select.txt\"):\n",
    "    auc_max = 0\n",
    "    number_feature_to_select = min_feature\n",
    "    text_file = open(savelink, \"a\")\n",
    "\n",
    "    for i in range(min_feature, max_feature, sequence):\n",
    "        text = \"n_features_to_select = \" + str(i) + \" \\n\"\n",
    "        n = text_file.writelines(text)\n",
    "\n",
    "        rfe_rf, rf_reduce, auc_score_rf_reduce = RFE_RF(X_train, y_train, X_test, y_test, i)\n",
    "        text = \"auc = \" + str(auc_score_rf_reduce) + \" \\n\"\n",
    "        n = text_file.writelines(text)\n",
    "        \n",
    "        if(auc_max < auc_score_rf_reduce):\n",
    "            auc_max = auc_score_rf_reduce\n",
    "            number_feature_to_select = i\n",
    "    \n",
    "    text_file.close()\n",
    "    return number_feature_to_select, auc_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b195d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number_feature_to_select, auc_max = optimal_number_feature_to_select(100, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0290050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_feature_to_select = 390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "738d2867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 605 features.\n",
      "Fitting estimator with 604 features.\n",
      "Fitting estimator with 603 features.\n",
      "Fitting estimator with 602 features.\n",
      "Fitting estimator with 601 features.\n",
      "Fitting estimator with 600 features.\n",
      "Fitting estimator with 599 features.\n",
      "Fitting estimator with 598 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 596 features.\n",
      "Fitting estimator with 595 features.\n",
      "Fitting estimator with 594 features.\n",
      "Fitting estimator with 593 features.\n",
      "Fitting estimator with 592 features.\n",
      "Fitting estimator with 591 features.\n",
      "Fitting estimator with 590 features.\n",
      "Fitting estimator with 589 features.\n",
      "Fitting estimator with 588 features.\n",
      "Fitting estimator with 587 features.\n",
      "Fitting estimator with 586 features.\n",
      "Fitting estimator with 585 features.\n",
      "Fitting estimator with 584 features.\n",
      "Fitting estimator with 583 features.\n",
      "Fitting estimator with 582 features.\n",
      "Fitting estimator with 581 features.\n",
      "Fitting estimator with 580 features.\n",
      "Fitting estimator with 579 features.\n",
      "Fitting estimator with 578 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 576 features.\n",
      "Fitting estimator with 575 features.\n",
      "Fitting estimator with 574 features.\n",
      "Fitting estimator with 573 features.\n",
      "Fitting estimator with 572 features.\n",
      "Fitting estimator with 571 features.\n",
      "Fitting estimator with 570 features.\n",
      "Fitting estimator with 569 features.\n",
      "Fitting estimator with 568 features.\n",
      "Fitting estimator with 567 features.\n",
      "Fitting estimator with 566 features.\n",
      "Fitting estimator with 565 features.\n",
      "Fitting estimator with 564 features.\n",
      "Fitting estimator with 563 features.\n",
      "Fitting estimator with 562 features.\n",
      "Fitting estimator with 561 features.\n",
      "Fitting estimator with 560 features.\n",
      "Fitting estimator with 559 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 556 features.\n",
      "Fitting estimator with 555 features.\n",
      "Fitting estimator with 554 features.\n",
      "Fitting estimator with 553 features.\n",
      "Fitting estimator with 552 features.\n",
      "Fitting estimator with 551 features.\n",
      "Fitting estimator with 550 features.\n",
      "Fitting estimator with 549 features.\n",
      "Fitting estimator with 548 features.\n",
      "Fitting estimator with 547 features.\n",
      "Fitting estimator with 546 features.\n",
      "Fitting estimator with 545 features.\n",
      "Fitting estimator with 544 features.\n",
      "Fitting estimator with 543 features.\n",
      "Fitting estimator with 542 features.\n",
      "Fitting estimator with 541 features.\n",
      "Fitting estimator with 540 features.\n",
      "Fitting estimator with 539 features.\n",
      "Fitting estimator with 538 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 536 features.\n",
      "Fitting estimator with 535 features.\n",
      "Fitting estimator with 534 features.\n",
      "Fitting estimator with 533 features.\n",
      "Fitting estimator with 532 features.\n",
      "Fitting estimator with 531 features.\n",
      "Fitting estimator with 530 features.\n",
      "Fitting estimator with 529 features.\n",
      "Fitting estimator with 528 features.\n",
      "Fitting estimator with 527 features.\n",
      "Fitting estimator with 526 features.\n",
      "Fitting estimator with 525 features.\n",
      "Fitting estimator with 524 features.\n",
      "Fitting estimator with 523 features.\n",
      "Fitting estimator with 522 features.\n",
      "Fitting estimator with 521 features.\n",
      "Fitting estimator with 520 features.\n",
      "Fitting estimator with 519 features.\n",
      "Fitting estimator with 518 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 516 features.\n",
      "Fitting estimator with 515 features.\n",
      "Fitting estimator with 514 features.\n",
      "Fitting estimator with 513 features.\n",
      "Fitting estimator with 512 features.\n",
      "Fitting estimator with 511 features.\n",
      "Fitting estimator with 510 features.\n",
      "Fitting estimator with 509 features.\n",
      "Fitting estimator with 508 features.\n",
      "Fitting estimator with 507 features.\n",
      "Fitting estimator with 506 features.\n",
      "Fitting estimator with 505 features.\n",
      "Fitting estimator with 504 features.\n",
      "Fitting estimator with 503 features.\n",
      "Fitting estimator with 502 features.\n",
      "Fitting estimator with 501 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "classes model: \n",
      "[1 2]\n",
      "[[0.59 0.41]\n",
      " [0.59 0.41]\n",
      " [0.54 0.46]\n",
      " [0.46 0.54]\n",
      " [0.54 0.46]\n",
      " [0.5  0.5 ]\n",
      " [0.42 0.58]\n",
      " [0.63 0.37]\n",
      " [0.61 0.39]\n",
      " [0.55 0.45]\n",
      " [0.52 0.48]\n",
      " [0.66 0.34]\n",
      " [0.46 0.54]\n",
      " [0.55 0.45]\n",
      " [0.55 0.45]\n",
      " [0.5  0.5 ]\n",
      " [0.53 0.47]\n",
      " [0.49 0.51]\n",
      " [0.48 0.52]\n",
      " [0.38 0.62]\n",
      " [0.57 0.43]\n",
      " [0.62 0.38]\n",
      " [0.54 0.46]\n",
      " [0.46 0.54]\n",
      " [0.6  0.4 ]\n",
      " [0.49 0.51]\n",
      " [0.59 0.41]\n",
      " [0.56 0.44]]\n"
     ]
    }
   ],
   "source": [
    "rfe, rf, fpr, tpr, thresholds, auc, accuracy, recall, confusion = RFE_RF(X_train, y_train, X_test, y_test, number_feature_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9c28723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62, 0.62, 0.58, 0.54, 0.52, 0.51, 0.5 , 0.47, 0.45, 0.44, 0.43,\n",
       "       0.41, 0.4 , 0.39, 0.38, 0.34])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "153f4632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd27c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.0625, 0.125 , 0.25  , 0.25  , 0.25  , 0.3125, 0.4375,\n",
       "       0.8125, 0.8125, 0.875 , 0.875 , 0.9375, 0.9375, 1.    , 1.    ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1b39a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6223a176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.08333333, 0.16666667,\n",
       "       0.33333333, 0.41666667, 0.41666667, 0.41666667, 0.5       ,\n",
       "       0.5       , 0.75      , 0.75      , 0.83333333, 0.83333333,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "247f45c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c81cc2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4],\n",
       "       [12,  4]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5320ab55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_feature = []\n",
    "# auc_arr = []\n",
    "# accuracy_arr = []\n",
    "# recall_arr = []\n",
    "# for i in range(50, 550, 50):\n",
    "#     number_feature_to_select = i\n",
    "#     rfe, rf, fpr, tpr, thresholds, auc, accuracy, recall,confusion = RFE_RF(X_train, y_train, X_test, y_test, number_feature_to_select)\n",
    "#     print(i)\n",
    "#     n_feature.append(i)\n",
    "#     auc_arr.append(auc)\n",
    "#     accuracy_arr.append(accuracy)\n",
    "#     recall_arr.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37769cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d194f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(auc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efcb81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accuracy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80159b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(recall_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb0a76fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features = np.arange(0, X_train.shape[1], 1, dtype=int)\n",
    "# selected_features = np.array(features)[rfe.get_support()]\n",
    "# selected_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
